{
 "cells": [
  {
   "cell_type": "raw",
   "id": "41777204-7c77-4e4a-b24f-c4b9720f3304",
   "metadata": {},
   "source": [
    "The below code is just to be run once to reformat the CSV file to be more useable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd7e49d-a0b4-451c-9dd6-7a60187f98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# Purpose of this function is to remove leading zeros from the filename column\n",
    "# and change extension from .ogg to .wav\n",
    "def process_filenames(df, column_name):\n",
    "    \"\"\"\n",
    "    Purpose of this function is to remove leading zeros from the values in the \n",
    "    filename column and to change the file extension from ogg to wav\n",
    "    \"\"\"\n",
    "    def modify_filename(filename):\n",
    "        # Split the filename and extension\n",
    "        name, ext = filename.rsplit('.', 1)\n",
    "        # Remove leading zeros and change the extension\n",
    "        if ext.lower() == 'ogg':\n",
    "            name = str(int(name))  # Convert to int to remove leading zeros, then back to string\n",
    "            return f\"{name}.wav\"\n",
    "        return filename  # Return unchanged if not .ogg\n",
    "    \n",
    "    # Apply the modification to the specified column\n",
    "    df[column_name] = df[column_name].apply(modify_filename)\n",
    "    return df\n",
    "    \n",
    "train_csv = \"music-classification-wav/versions/3/IA/train.csv\"\n",
    "genres_csv = \"music-classification-wav/versions/3/IA/genres.csv\"\n",
    "train_df = pd.read_csv(train_csv)\n",
    "genres_df = pd.read_csv(genres_csv)\n",
    "\n",
    "\n",
    "## Process the DataFrame\n",
    "processed_df = process_filenames(train_df, 'filename')\n",
    "processed_df = processed_df.drop(columns=[\"filepath\"])\n",
    "#processed_df.to_csv(\"processed_train.csv\")\n",
    "#print(processed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b8f9aa-4583-4f09-a2a5-d0df34918f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Entries: 19922\n",
      "Actual Files: 19896\n",
      "CSV has 26 more entries than we have files.\n",
      "\n",
      "cleaning...\n",
      "\n",
      "Size before cleaning: 19922\n",
      "Size after cleaning: 19896\n",
      "26 entries deleted!\n",
      "entries deleted:\n",
      "\t 2549.wav\n",
      "\t 3137.wav\n",
      "\t 20407.wav\n",
      "\t 11088.wav\n",
      "\t 16312.wav\n",
      "\t 1239.wav\n",
      "\t 7795.wav\n",
      "\t 19765.wav\n",
      "\t 24899.wav\n",
      "\t 4040.wav\n",
      "\t 8897.wav\n",
      "\t 9963.wav\n",
      "\t 15980.wav\n",
      "\t 22698.wav\n",
      "\t 2643.wav\n",
      "\t 17475.wav\n",
      "\t 13702.wav\n",
      "\t 23369.wav\n",
      "\t 23078.wav\n",
      "\t 17940.wav\n",
      "\t 22295.wav\n",
      "\t 20462.wav\n",
      "\t 952.wav\n",
      "\t 20445.wav\n",
      "\t 3071.wav\n",
      "\t 13954.wav\n",
      "Wrote csv file to cleaned_train.csv\n"
     ]
    }
   ],
   "source": [
    "# # This is a df where \"filename.ogg\" has been changed to \"filename.wav\".\n",
    "# cleaned_df = pd.read_csv(\"processed.csv\", index_col=0)  \n",
    "cleaned_df = processed_df.copy()\n",
    "# Now I want to check this CSV against the files we actually have \n",
    "# and remove any entries from the CSV that don't have a corresponding file\n",
    "audio_dir = pathlib.Path(\"music-classification-wav/versions/3/IA/train/\")\n",
    "audio_files = audio_dir.iterdir()\n",
    "actual_filenames = [file.name for file in audio_files]\n",
    "\n",
    "\n",
    "csv_filenames = cleaned_df.loc[:, \"filename\"]\n",
    "print(f\"CSV Entries: {len(csv_filenames)}\")\n",
    "print(f\"Actual Files: {len(actual_filenames)}\")\n",
    "print(f\"CSV has {len(csv_filenames) - len(actual_filenames)} more entries than we have files.\\n\")\n",
    "size_before = cleaned_df.shape[0]\n",
    "print(\"cleaning...\\n\")\n",
    "deleted_entries_count = 0\n",
    "deleted_entries_list = []\n",
    "for filename in csv_filenames:\n",
    "    if filename not in actual_filenames:\n",
    "        deleted_entries_count += 1\n",
    "        deleted_entries_list.append(filename)\n",
    "        cleaned_df = cleaned_df[cleaned_df['filename'] != filename]\n",
    "        \n",
    "print(f\"Size before cleaning: {size_before}\")\n",
    "print(f\"Size after cleaning: {cleaned_df.shape[0]}\")\n",
    "print(f\"{deleted_entries_count} entries deleted!\")\n",
    "print(f\"entries deleted:\")\n",
    "[print(\"\\t\", file) for file in deleted_entries_list]\n",
    "cleaned_csv_path = \"cleaned_train.csv\"\n",
    "cleaned_df.to_csv(cleaned_csv_path)\n",
    "print(f\"Wrote csv file to {cleaned_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42b875-dfe3-4d98-9e30-0d7d367e4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "If the csv file created by the above code has already been generated, you can start here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b92429-6225-43ba-9a4e-6a11889d906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rock: 3095\n",
      "Electronic: 3071\n",
      "Punk: 2582\n",
      "Experimental: 1799\n",
      "Hip-Hop: 1756\n",
      "Folk: 1214\n",
      "Chiptune / Glitch: 1181\n",
      "Instrumental: 1044\n",
      "Pop: 944\n",
      "International: 814\n",
      "Ambient Electronic: 796\n",
      "Classical: 495\n",
      "Old-Time / Historic: 408\n",
      "Jazz: 306\n",
      "Country: 142\n",
      "Soul-RnB: 94\n",
      "Spoken: 94\n",
      "Blues: 58\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cleaned_df = pd.read_csv(\"cleaned_train.csv\")\n",
    "# # Find how many examples in each genre.\n",
    "# cleaned_csv_path = \"cleaned_train.csv\"\n",
    "# cleaned_df = pd.read_csv(cleaned_csv_path, index_col=0)\n",
    "freq_dict = cleaned_df[\"genre\"].value_counts().to_dict()\n",
    "for key, value in freq_dict.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f8f8ad-53a1-4420-b0f7-07bf4c5584e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 95.75 seconds\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "# Function to extract Mel Spectrogram features\n",
    "def extract_mel_spectrogram(file_path, n_mels=64, duration=5, sr=22050):\n",
    "    y, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / mel_spec_db.std()\n",
    "    return mel_spec_db.T  # Transpose to get (time, features)\n",
    "\n",
    "audio_dir = pathlib.Path(\"music-classification-wav/versions/3/IA/train/\")\n",
    "\n",
    "audio_files = [str(audio_dir / filename) for filename in cleaned_df.loc[:, \"filename\"]]\n",
    "\n",
    "start_time = time.time()\n",
    "features = [extract_mel_spectrogram(file) for file in audio_files]\n",
    "features = [torch.tensor(f, dtype=torch.float32) for f in features]\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0f32b7-48b5-4f15-bc1b-5eca31a1955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('features.pkl', 'wb') as file:\n",
    "    pickle.dump(features, file)\n",
    "\n",
    "with open('features.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de61c0cd-5924-4438-ab14-c644c1dadfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deus/Development/JupyterNotebooks/audio-clusters/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [216, 64] at entry 0 and [22, 64] at entry 6182",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Convert the list of features to a batch\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [216, 64] at entry 0 and [22, 64] at entry 6182"
     ]
    }
   ],
   "source": [
    "class AudioTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=4):\n",
    "        super(AudioTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        x = self.pooling(x).squeeze(-1)  # (batch_size, d_model)\n",
    "        return x\n",
    "        \n",
    "start_time = time.time()\n",
    "# Instantiate the Transformer model\n",
    "input_dim = features[0].shape[1]\n",
    "transformer = AudioTransformer(input_dim=input_dim)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Convert the list of features to a batch\n",
    "features = torch.stack(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c64866-e237-4c09-bcce-b818c0fe1516",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Instantiate the Transformer model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     20\u001b[0m transformer \u001b[38;5;241m=\u001b[39m AudioTransformer(input_dim\u001b[38;5;241m=\u001b[39minput_dim)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Convert the list of features to a batch\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "class AudioTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=4):\n",
    "        super(AudioTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        x = self.pooling(x).squeeze(-1)  # (batch_size, d_model)\n",
    "        return x\n",
    "\n",
    "# Instantiate the Transformer model\n",
    "input_dim = features[0].shape[1]\n",
    "transformer = AudioTransformer(input_dim=input_dim)\n",
    "\n",
    "# Convert the list of features to a batch\n",
    "features = torch.stack(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d62c6-94ad-4a8c-9ea1-bbbd46d8bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = transformer(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794fc77d-7e9a-499e-afa3-ae640be8f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 5\n",
    "embeddings_np = embeddings.numpy()\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(embeddings_np)\n",
    "cluster_centers = kmeans.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c487b70-3573-4a5c-a709-383e491c42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(nn.Module):\n",
    "    def __init__(self, cluster_centers):\n",
    "        super(ClusteringLayer, self).__init__()\n",
    "        self.cluster_centers = nn.Parameter(torch.tensor(cluster_centers, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = 1.0 / (1.0 + torch.sum((x.unsqueeze(1) - self.cluster_centers) ** 2, dim=2))\n",
    "        q = q / torch.sum(q, dim=1, keepdim=True)\n",
    "        return q\n",
    "\n",
    "# Define the DEC model\n",
    "class DECModel(nn.Module):\n",
    "    def __init__(self, transformer, cluster_centers):\n",
    "        super(DECModel, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.clustering_layer = ClusteringLayer(cluster_centers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.transformer(x)\n",
    "        q = self.clustering_layer(embeddings)\n",
    "        return q\n",
    "\n",
    "# Instantiate the DEC model\n",
    "dec_model = DECModel(transformer, cluster_centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267fffc6-4cce-4dc5-b2f6-25d75f9ac5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def kl_divergence_loss(p, q):\n",
    "    return torch.sum(p * torch.log(p / (q + 1e-10)))\n",
    "\n",
    "# Target distribution function\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / torch.sum(q, dim=0)\n",
    "    return (weight.T / torch.sum(weight, dim=1)).T\n",
    "\n",
    "# Fine-tuning with DEC\n",
    "optimizer = optim.Adam(dec_model.parameters(), lr=1e-4)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    q = dec_model(features)\n",
    "    p = target_distribution(q).detach()\n",
    "\n",
    "    loss = kl_divergence_loss(p, q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3619f-fb5b-44d3-a16d-330d415cb84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    final_q = dec_model(features)\n",
    "    final_assignments = torch.argmax(final_q, dim=1).numpy()\n",
    "\n",
    "print(\"Final cluster assignments:\", final_assignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7b36b-19d4-40b2-ae46-69b742036bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
